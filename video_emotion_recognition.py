# -*- coding: utf-8 -*-
"""Video emotion recognition.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18kkReiesi4pEYBRsYZLSJ-pfbRK-l2Ga
"""

import os
import urllib.request

# Example for Actor 01 (adjust URL for other actors)
url = "https://zenodo.org/record/1188976/files/Video_Speech_Actor_01.zip?download=1"
os.makedirs("ravdess_video", exist_ok=True)
urllib.request.urlretrieve(url, "ravdess_video/Actor01.zip")

!unzip -q ravdess_video/Actor01.zip -d ravdess_video/Actor01

!pip install timm einops opencv-python decord
!pip install -q \
    peft==0.10.0 \
    transformers==4.37.2 \
    tokenizers==0.15.2 \
    huggingface_hub==0.33.4 \
    datasets==2.19.1 \
    sentence-transformers==2.6.1 \
    accelerate==0.28.0 \
    torchaudio

import os
import numpy as np
from PIL import Image
from datasets import Dataset, DatasetDict
from decord import VideoReader, cpu
from transformers import (
    VideoMAEImageProcessor,
    TimesformerForVideoClassification,
    TrainingArguments,
    Trainer,
    DefaultDataCollator
)
import torch

def extract_label_from_path(path):
    # Example filename: 01-01-01-01-01-01-01.mp4
    # Emotion is at position 3 (index 2), mapping below:
    emotion_map = {
        "01": "neutral", "02": "calm", "03": "happy", "04": "sad",
        "05": "angry", "06": "fearful", "07": "disgust", "08": "surprised"
    }
    emotion_id = path.split("/")[-1].split("-")[2]
    return emotion_map.get(emotion_id, "unknown")

from glob import glob

video_paths = glob("/content/ravdess_video/**/*.mp4", recursive=True)

data = {"video_path": [], "label": []}
for path in video_paths:
    label = extract_label_from_path(path)
    if label != "unknown":
        data["video_path"].append(path)
        data["label"].append(label)

label2id = {label: idx for idx, label in enumerate(sorted(set(data["label"])))}
data["label"] = [label2id[label] for label in data["label"]]

dataset = Dataset.from_dict(data).train_test_split(test_size=0.2)

image_processor = VideoMAEImageProcessor.from_pretrained("facebook/timesformer-base-finetuned-k400")

def load_video_frames(video_path, num_frames=8):
    vr = VideoReader(video_path, ctx=cpu(0))
    total_frames = len(vr)
    indices = np.linspace(0, total_frames - 1, num=num_frames, dtype=int)
    frames = vr.get_batch(indices).asnumpy()  # shape: (num_frames, H, W, 3)
    pil_frames = [Image.fromarray(frame) for frame in frames]
    return pil_frames

def transform(example):
    frames = load_video_frames(example["video_path"])
    inputs = image_processor(frames, return_tensors="pt")
    example["pixel_values"] = inputs["pixel_values"].squeeze(0)  # (T, 3, H, W)
    return example

dataset = dataset.map(transform, remove_columns=["video_path"])

from transformers import TimesformerModel, TimesformerConfig, TimesformerForVideoClassification, TrainingArguments, Trainer
import torch.nn as nn
import numpy as np

# Load base model configuration
config = TimesformerConfig.from_pretrained("facebook/timesformer-base-finetuned-k400")
config.num_labels = len(label2id)
config.label2id = label2id
config.id2label = {v: k for k, v in label2id.items()}

# Load base model (without classification head)
base_model = TimesformerModel.from_pretrained("facebook/timesformer-base-finetuned-k400", config=config)

# Wrap it into a new classifier model manually
class CustomTimesformerForClassification(TimesformerForVideoClassification):
    def __init__(self, config):
        super().__init__(config)
        self.timesformer = base_model
        self.classifier = nn.Linear(config.hidden_size, config.num_labels)

# Initialize the model
model = CustomTimesformerForClassification(config)

# Training args
training_args = TrainingArguments(
    output_dir="./results",
    per_device_train_batch_size=2,
    per_device_eval_batch_size=2,
    num_train_epochs=3,
    evaluation_strategy="epoch",
    save_strategy="epoch",
    logging_dir="./logs",
    learning_rate=5e-5,
    load_best_model_at_end=True,
    metric_for_best_model="accuracy",
    logging_steps=10,
)

# Accuracy metric
def compute_metrics(p):
    preds = np.argmax(p.predictions, axis=1)
    labels = p.label_ids
    acc = np.mean(preds == labels)
    return {"accuracy": acc}

# Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=dataset["train"],
    eval_dataset=dataset["test"],
    compute_metrics=compute_metrics,
)

# Train
trainer.train()

metrics = trainer.evaluate()
print("Test Accuracy:", metrics["eval_accuracy"])